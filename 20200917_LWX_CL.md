# <center>20200917_LWX_CL</center>

### 1. 心得

- 勿用试探性语气，不自信的表现
- Research延展性，平时多积累
- 适当说出自己的想法，华为需要+自己hold住
- 下一步可以怎么做，探索一些新的方向、问题
- 类似问题想听听你的意见
- assignment都是给定不变的，对产品线的理解不是很明确，把握的不是很准，猜
- 数通是HW一个部门，产品线，和法国研究所在做问题，他们用AI做的，本身计算量也大

### 2.  CT-Car

Car：commit access rate，控制上送到控制面的包数量，每个包有相应的host和Protocol（哪个协议（BGP，OSPF，ISF），不同场景支持协议个数不同，每个业务有多个session，其中session是建立协议的对端，如BGP neighbor，peer，对等体（数目随机））

- 用户car：session级别的commit access rate，某用户发了100万数据报文，可能攻击我们，另一个用户发了10个，可能丢，需要公平处理两个用户；
- 协议car：需要公平处理，BGP，AIP可能发生风暴，一个报文多，一个报文少，处理报文多的协议，其他协议可能没有处理时间，CPU time；
- 有对应关系固定，每个Protocol对应一个CPU, Host-Protocol-CPU （80%，。。。）,host队列出来进入Protocol队列

流程：

- 转发芯片（硬件，但需要设计相应的软件算法），路由器是一台电脑，有自己的CPU和rate，access是device，以TCP/IP连载一起，路由器和转发器run，报文通过系统网卡上传到CPU上，交换机网卡跑EK，为了高性能，把scheduler上送的报文放到网卡的buffer里面，把buffer DMA到主内存去，然后CPU一直读主内存，进行协议处理。只考虑一个设备，一个路由器或交换机，其架构是 __接受周围路由器基于某些协议的信息，转化成路由表，下发到转发芯片__
- 处理报文的芯片，芯片转发流量，识别协议，识别是BGP报文，需要上送控制面去处理，如路由导致路由表变化，需要CPU处理，生成新的路由表，下发到转发芯片上。流量经过转发芯片，识别流量，进行异常流量过滤，智能排障算法（统计、机器学习生成黑名单白名单），硬件卸载（逻辑简单的），需要计算量的报文如BGP，进入不同的用户队列和协议队列，两级队列，由硬件调度报文出来上发到OS进行处理
- 令牌桶的深度：应对突发的情况，令牌会堆积，如果一段时间没流量来回堆积，如果限制是1M，实际来包速率是0.5M，这样就会造成令牌堆积，如果没有深度，多余的令牌就会丢掉。走过去看有没有令牌，有令牌才能走，没令牌需要等，如高速公路过收费亭，如果汽车走的快，导致令牌消耗的快，后面来的汽车需要等产生令牌之后才能通信。数据包进入一个队列，有些队列没有缓存，有些有buffer，有一个关卡，即令牌桶，往桶里不断灌令牌，灌令牌速率相当于对 队列出去的速率设置关卡，灌令牌速率1M，出去的速率只能1M，深度是桶里最多放多少令牌，没走一个数据包去一个令牌，__调的参数：灌令牌速率和深度，出来的速率取决于灌令牌速率，出来后就上送到主控板的CPU__

多时钟标签算法，进去的时候打标签，出来的时候根据公平性指数进行调度。

协议限速：限的是上送的速率，从转发芯片送到操作系统的速率
上传的包，看其公平，协议上的公平

- 限速标签：最大可能限速的导数
- 权重标签：每进来一个包增加权重导数，等于出队列的virtual time

困难：

- 协议限速，令牌桶方案，有个deps，CIR commit information rate
- 两级令牌桶，一边多一边少，CIR、CBS和流量真实情况对不上，产生non-work-conservation，CPU不饱和，公平性保证弱，只能case-by-case去调整、识别场景，令牌桶深度不够深、rate不够高，手工调，不合理
- 限速：主要转发芯片上提供了硬件的令牌桶，非常快，要求软件也要快，令牌桶参数配置，看看能否用lyapunov建立模型

问题：

- 调度：数据包如何出去，什么时候让哪个包上送，push多少和资源利用率不太匹配，但这里不知道主控板的CPU占用率怎么样
- 虽然可以公平，但每个协议处理能力不同或对应报文的CPU overhead/计算量不同，把各个协议当成黑盒（协议处理逻辑，从buffer取出来，放到自己进程处理，BGP router process进程，占用CPU内存多少，当成整体，不需要管），观察其表现，看各个协议的内存或CPU占用率这些外部feature，根据这个调整外部参数，转发芯片上进行反压（目前处理这个协议的CPU利用率太高了，需要让过来的包少一点，使其可以正常工作）
- 把CPU当做一个黑盒，知道CPU利用率的信息，隔三差五获取一下，调的目的，最终让这一排CPU利用效率最多，但处理业务取决于送来的数据量对CPU消耗多少计算量，有影响，所以如何上送给它，产生计算量，消耗CPU，得到一些信息，反馈给如何做数据调整，使CPU一直达到80%以下工作
- 不同协议有不同队列，队列有scheduler（设计算法），通过观测CPU core或每个进程feature调整算法参数，scheduler能调的参数：每个令牌桶的深度和CIR commit information rate，每个桶有两个参数，有上千个桶，问题是有个大的bottleneck，如果一个协议很忙（CPU占用率很高），无法动态回去把速度限下来，权重应该动态调整
- 间接手段调，这个包一定是去这里，但一段时间内有多少包去这里待定
- 调度行为是延时的，当前的调度决策一段时间才能effect，或者限速的效果不是及时的
  two-time-scale，两个时间尺度: 令牌桶的配置时间尺度大些，深度和速度（平均），时间尺度较大，秒级别的调整, 时间尺度小的是 单个包的decision？
- fairness：一段时间内，两个协议的权重比是2：1，在1s内让2/3数据包走第一个协议， 1/3走第二个协议，保证协议出去（上送）的数据包数量或流量是2：1

算法：

- 输入：CPU对每个协议处理的进程的CPU占用率
- 输出：下面4000个用户队列，4000个协议队列分别的限速值，CIR 令牌桶深度和前令牌的速度，但是要决定几千个，决定时需考虑协议间和用户间的公平性，规模大
- buffer可以看成一个queue，需要一套机制去反压，保持buffer在某一个长度，或保持CPU阈值，如80%以下（virtual queue）。用lyapunov，协议之前公平性如何保证，每个协议对应一个CPU core，可以多个协议对一个。
- lyapunov：核心思想backpressure，反压，根据过去调度的过程动态调整，online control，decision making，每个iteration输出速度，求解优化问题



下面影响control plane只是通过协议限速，中间加个CPU做调度可能更好些，两个CPU通过某种方式交互下信息

假设前面有个负责分发，将其push到相应的协议处理的核的buffer，一直去pull buffer，如果有包拿出来，这是保持高通量的手段

另一个方案，加一个CPU，根据上面的信息动态取包，直接进行matching的decision

如果加CPU，是否可以用lyapunov作matching，CPU做限速，哪个时间出哪个包

假设可以获取，基本模型看看能否跟现在的问题对应上
难点：需要加一个CPU，获取主控板的信息



###限流算法

- 场景

抢购--接口，服务器--崩溃

限流主要是对超过限制数的流量有影响，当请求量没有超过限制值时就是正常的。
比如限流设置的是处理100 个每秒，当请求量达到每秒150个的时候，还是能正常处理100 个请求，剩余50 个根据业务场景可以选择直接返回不处理。

- 漏桶

数据包 b达到，上个包到达时间a_t

桶capacity为C，满则溢出，桶当前的包数为w

流出速度为 r，平滑流量

> when (b),

> ​	b_t=now

> ​	w_b=(b_t-a_t)*r

> ​	w=min (w-w_b,0)

> ​	if (w<c)

> ​		w++

> ​		return true

> ​	else

> ​		return false

- 令牌桶

token到达速率 r

桶capacity C，当前桶内的token数 w，突发流量

数据包 b达到，上个包到达时间a_t，取走token处理请求，否则拒绝服务

两种形式

5/s----5/s

20--4s--20/s

> When (b)

> ​	b_t=now

> ​	w_b=(b_t-a_t)*r

> ​	w=min (w+w_b,C)

> ​	if (w>0)

>  	w--

> ​		return true

> ​	else

> ​		return false	

- 比较

​					漏桶			令牌桶

arrival     	不固定		固定

  桶						

 流出			固定			不固定

​					 整流			突发
